{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from transformers import TFBertModel, BertTokenizer, TFBertMainLayer, BertConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "    raise SystemError('GPU not found')\n",
    "print('found GPU at {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = TFBertModel.from_pretrained(\"bert-base-cased\")\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('../train_data/train_triple_all_signals.csv', delimiter=',')\n",
    "validation_data = pd.read_csv('../train_data/validation_triple_all_signals.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = train_data.head(100)\n",
    "# validation_data = validation_data.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_dummy = np.empty(len(train_data))\n",
    "Y_validation_dummy = np.empty(len(validation_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TOKENS = 125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids_anchor_all = []\n",
    "train_mask_anchor_all = []\n",
    "train_seg_anchor_all = []\n",
    "\n",
    "train_ids_true_all = []\n",
    "train_mask_true_all = []\n",
    "train_seg_true_all = []\n",
    "\n",
    "train_ids_false_all = []\n",
    "train_mask_false_all = []\n",
    "train_seg_false_all = []\n",
    "\n",
    "for i,row in tqdm(train_data.iterrows()):\n",
    "    \n",
    "    anchor_catch_all = str(row['article_page_title']) + str(row['article_page_meta_description']) + str(row['article_page_keywords'])\n",
    "    \n",
    "    #encoder article title\n",
    "    return_tokenizer1 = bert_tokenizer.encode_plus(\n",
    "      anchor_catch_all,\n",
    "      max_length=MAX_TOKENS,\n",
    "      add_special_tokens=True,\n",
    "      return_token_type_ids=True,\n",
    "      pad_to_max_length=True,\n",
    "      return_attention_mask=True,\n",
    "    )\n",
    "    \n",
    "    train_ids_anchor_all.append(return_tokenizer1['input_ids'])\n",
    "    train_mask_anchor_all.append(return_tokenizer1['attention_mask'])\n",
    "    train_seg_anchor_all.append(return_tokenizer1['token_type_ids'])  \n",
    "    \n",
    "    \n",
    "    true_catch_all = str(row['true_table_page_title']) + str(row['true_table_page_summary']) + str(row['true_table_page_keywords'])\n",
    "    \n",
    "    #encoder table true title\n",
    "    return_tokenizer2 = bert_tokenizer.encode_plus(\n",
    "      true_catch_all,\n",
    "      max_length=MAX_TOKENS,\n",
    "      add_special_tokens=True,\n",
    "      return_token_type_ids=True,\n",
    "      pad_to_max_length=True,\n",
    "      return_attention_mask=True,\n",
    "    )\n",
    "    \n",
    "    train_ids_true_all.append(return_tokenizer2['input_ids'])\n",
    "    train_mask_true_all.append(return_tokenizer2['attention_mask'])\n",
    "    train_seg_true_all.append(return_tokenizer2['token_type_ids'])    \n",
    "    \n",
    "    false_catch_all = str(row['false_table_page_title']) + str(row['false_table_page_summary']) + str(row['false_table_page_keywords'])\n",
    "    \n",
    "    #encoder table true false\n",
    "    return_tokenizer3 = bert_tokenizer.encode_plus(\n",
    "      false_catch_all,\n",
    "      max_length=MAX_TOKENS,\n",
    "      add_special_tokens=True,\n",
    "      return_token_type_ids=True,\n",
    "      pad_to_max_length=True,\n",
    "      return_attention_mask=True,\n",
    "    )\n",
    "    \n",
    "    train_ids_false_all.append(return_tokenizer3['input_ids'])\n",
    "    train_mask_false_all.append(return_tokenizer3['attention_mask'])\n",
    "    train_seg_false_all.append(return_tokenizer3['token_type_ids'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids_anchor_all = np.asarray(train_ids_anchor_all)\n",
    "train_mask_anchor_all = np.asarray(train_mask_anchor_all)\n",
    "train_seg_anchor_all = np.asarray(train_seg_anchor_all)\n",
    "\n",
    "train_ids_true_all = np.asarray(train_ids_true_all)\n",
    "train_mask_true_all = np.asarray(train_mask_true_all)\n",
    "train_seg_true_all = np.asarray(train_seg_true_all)\n",
    "\n",
    "train_ids_false_all = np.asarray(train_ids_false_all)\n",
    "train_mask_false_all = np.asarray(train_mask_false_all)\n",
    "train_seg_false_all = np.asarray(train_seg_false_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ids_anchor_all = []\n",
    "val_mask_anchor_all = []\n",
    "val_seg_anchor_all = []\n",
    "\n",
    "val_ids_true_all = []\n",
    "val_mask_true_all = []\n",
    "val_seg_true_all = []\n",
    "\n",
    "val_ids_false_all = []\n",
    "val_mask_false_all = []\n",
    "val_seg_false_all = []\n",
    "\n",
    "for i,row in tqdm(validation_data.iterrows()):\n",
    "    \n",
    "    anchor_catch_all = str(row['article_page_title']) + str(row['article_page_meta_description']) + str(row['article_page_keywords'])\n",
    "    \n",
    "    #encoder article title\n",
    "    return_tokenizer1 = bert_tokenizer.encode_plus(\n",
    "      anchor_catch_all,\n",
    "      max_length=MAX_TOKENS,\n",
    "      add_special_tokens=True,\n",
    "      return_token_type_ids=True,\n",
    "      pad_to_max_length=True,\n",
    "      return_attention_mask=True,\n",
    "    )\n",
    "    \n",
    "    val_ids_anchor_all.append(return_tokenizer1['input_ids'])\n",
    "    val_mask_anchor_all.append(return_tokenizer1['attention_mask'])\n",
    "    val_seg_anchor_all.append(return_tokenizer1['token_type_ids'])  \n",
    "    \n",
    "    \n",
    "    true_catch_all = str(row['true_table_page_title']) + str(row['true_table_page_summary']) + str(row['true_table_page_keywords'])\n",
    "    \n",
    "    #encoder table true title\n",
    "    return_tokenizer2 = bert_tokenizer.encode_plus(\n",
    "      true_catch_all,\n",
    "      max_length=MAX_TOKENS,\n",
    "      add_special_tokens=True,\n",
    "      return_token_type_ids=True,\n",
    "      pad_to_max_length=True,\n",
    "      return_attention_mask=True,\n",
    "    )\n",
    "    \n",
    "    val_ids_true_all.append(return_tokenizer2['input_ids'])\n",
    "    val_mask_true_all.append(return_tokenizer2['attention_mask'])\n",
    "    val_seg_true_all.append(return_tokenizer2['token_type_ids'])    \n",
    "    \n",
    "    false_catch_all = str(row['false_table_page_title']) + str(row['false_table_page_summary']) + str(row['false_table_page_keywords'])\n",
    "    \n",
    "    #encoder table true false\n",
    "    return_tokenizer3 = bert_tokenizer.encode_plus(\n",
    "      false_catch_all,\n",
    "      max_length=MAX_TOKENS,\n",
    "      add_special_tokens=True,\n",
    "      return_token_type_ids=True,\n",
    "      pad_to_max_length=True,\n",
    "      return_attention_mask=True,\n",
    "    )\n",
    "    \n",
    "    val_ids_false_all.append(return_tokenizer3['input_ids'])\n",
    "    val_mask_false_all.append(return_tokenizer3['attention_mask'])\n",
    "    val_seg_false_all.append(return_tokenizer3['token_type_ids'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ids_anchor_all = np.asarray(val_ids_anchor_all)\n",
    "val_mask_anchor_all = np.asarray(val_mask_anchor_all)\n",
    "val_seg_anchor_all = np.asarray(val_seg_anchor_all)\n",
    "\n",
    "val_ids_true_all = np.asarray(val_ids_true_all)\n",
    "val_mask_true_all = np.asarray(val_mask_true_all)\n",
    "val_seg_true_all = np.asarray(val_seg_true_all)\n",
    "\n",
    "val_ids_false_all = np.asarray(val_ids_false_all)\n",
    "val_mask_false_all = np.asarray(val_mask_false_all)\n",
    "val_seg_false_all = np.asarray(val_seg_false_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplet_loss(y_true, y_pred, alpha = 0.5):\n",
    "     \n",
    "    anchor = y_pred[0:,0:768]\n",
    "    positive = y_pred[0:,768:1536]\n",
    "    negative = y_pred[0:,1536:2304]\n",
    "        \n",
    "    # distance between the anchor and the positive\n",
    "    pos_dist = tf.keras.layers.Dot(axes=1,normalize=True)([anchor, positive])\n",
    "    \n",
    "    # distance between the anchor and the negative\n",
    "    neg_dist = tf.keras.layers.Dot(axes=1,normalize=True)([anchor, negative])\n",
    "    \n",
    "    # compute loss\n",
    "    basic_loss = (1 - pos_dist) - (1 - neg_dist) + alpha\n",
    "    loss = tf.keras.backend.maximum(basic_loss,0.0)\n",
    " \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_title_id = tf.keras.layers.Input(shape=(MAX_TOKENS,), name='input_token1', dtype='int32')\n",
    "article_title_mask_id = tf.keras.layers.Input(shape=(MAX_TOKENS,), name='masked_token1', dtype='int32')\n",
    "article_title_token_id = tf.keras.layers.Input(shape=(MAX_TOKENS,), name='token_ids_token1', dtype='int32')\n",
    "\n",
    "table_true_title_id = tf.keras.layers.Input(shape=(MAX_TOKENS,), name='input_token2', dtype='int32')\n",
    "table_true_title_mask_id = tf.keras.layers.Input(shape=(MAX_TOKENS,), name='masked_token2', dtype='int32')\n",
    "table_true_title_token_id = tf.keras.layers.Input(shape=(MAX_TOKENS,), name='token_ids_token2', dtype='int32')\n",
    "\n",
    "table_false_title_id = tf.keras.layers.Input(shape=(MAX_TOKENS,), name='input_token3', dtype='int32')\n",
    "table_false_title_mask_id = tf.keras.layers.Input(shape=(MAX_TOKENS,), name='masked_token3', dtype='int32')\n",
    "table_false_title_token_id = tf.keras.layers.Input(shape=(MAX_TOKENS,), name='token_ids_token3', dtype='int32')\n",
    "\n",
    "#bert model layers\n",
    "\n",
    "last_hidden_state1 = bert_model.bert([article_title_id,article_title_mask_id,article_title_token_id])\n",
    "last_hidden_state2 = bert_model.bert([table_true_title_id,table_true_title_mask_id,table_true_title_token_id])\n",
    "last_hidden_state3 = bert_model.bert([table_false_title_id,table_false_title_mask_id,table_false_title_token_id])\n",
    "\n",
    "out1 = tf.keras.backend.mean(last_hidden_state1[0], axis=1)\n",
    "out2 = tf.keras.backend.mean(last_hidden_state2[0], axis=1)\n",
    "out3 = tf.keras.backend.mean(last_hidden_state3[0], axis=1)\n",
    "\n",
    "concatenated = tf.keras.layers.Concatenate(axis=-1)([out1,out2,out3])\n",
    "\n",
    "model = tf.keras.Model(inputs=[article_title_id, \n",
    "                               article_title_mask_id,\n",
    "                               article_title_token_id,\n",
    "                               table_true_title_id,\n",
    "                               table_true_title_mask_id,\n",
    "                               table_true_title_token_id,\n",
    "                               table_false_title_id,\n",
    "                               table_false_title_mask_id,\n",
    "                               table_false_title_token_id], \n",
    "                       outputs = concatenated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=triplet_loss,optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit([train_ids_anchor_all,\n",
    "                     train_mask_anchor_all,\n",
    "                     train_seg_anchor_all,\n",
    "                     train_ids_true_all,\n",
    "                     train_mask_true_all,\n",
    "                     train_seg_true_all,\n",
    "                     train_ids_false_all,\n",
    "                     train_mask_false_all,\n",
    "                     train_seg_false_all], \n",
    "                    Y_train_dummy, \n",
    "                    epochs=5, \n",
    "                    batch_size=16,\n",
    "                    verbose=1,\n",
    "                    validation_data=([\n",
    "                     val_ids_anchor_all,\n",
    "                     val_mask_anchor_all,\n",
    "                     val_seg_anchor_all,\n",
    "                     val_ids_true_all,\n",
    "                     val_mask_true_all,\n",
    "                     val_seg_true_all,\n",
    "                     val_ids_false_all,\n",
    "                     val_mask_false_all,\n",
    "                     val_seg_false_all], Y_validation_dummy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"bert_encoder_model2\",save_format='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaded_model = tf.keras.models.load_model('bert_encoder_model', custom_objects={'triplet_loss': triplet_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer_name = 'bert'\n",
    "# intermediate_layer_model = tf.keras.Model(inputs=[loaded_model.input[0],loaded_model.input[1],loaded_model.input[2]],\n",
    "#                                  outputs=loaded_model.get_layer(layer_name).output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hid, out = intermediate_layer_model.predict([val_ids_anchor_all,val_mask_anchor_all,val_seg_anchor_all])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
