{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import random\n",
    "import gensim as gs\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer()\n",
    "import tensorflow as tf\n",
    "from transformers import TFBertModel, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "    raise SystemError('GPU not found')\n",
    "print('found GPU at {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = TFBertModel.from_pretrained(\"fine_tuning_bert\")\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TOKENS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = pd.read_csv('../../dataset/data_articles_test.csv', delimiter=',', keep_default_na=False)\n",
    "\n",
    "tables = pd.read_csv('../../dataset/distinct_tables_allsignals.csv', delimiter=',', keep_default_na=False)\n",
    "tables = tables.drop(tables[tables.table_page_title == \"\"].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## encode articles using encoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_ids = []\n",
    "input_ids_article_title = []\n",
    "input_masks_article_title = []\n",
    "input_token_id_article_title = []\n",
    "\n",
    "for i, row in tqdm(articles.iterrows()):\n",
    "    \n",
    "    article_page_title = row['page_title']\n",
    "    article_page_description = str(row['meta_description'])\n",
    "       \n",
    "    #fast text embedding\n",
    "    return_tokenizer1 = bert_tokenizer.encode_plus(\n",
    "      article_page_title,\n",
    "      article_page_description,\n",
    "      max_length=MAX_TOKENS,\n",
    "      add_special_tokens=True,\n",
    "      return_token_type_ids=True,\n",
    "      pad_to_max_length=True,\n",
    "      return_attention_mask=True,\n",
    "    )\n",
    "    \n",
    "    #save list\n",
    "    articles_ids.append(row['page_id'])\n",
    "    \n",
    "    input_ids_article_title.append(return_tokenizer1['input_ids'])\n",
    "    input_masks_article_title.append(return_tokenizer1['attention_mask'])\n",
    "    input_token_id_article_title.append(return_tokenizer1['token_type_ids']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_article_title = np.array(input_ids_article_title)\n",
    "input_masks_article_title = np.array(input_masks_article_title)\n",
    "input_token_id_article_title = np.array(input_token_id_article_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_token_id_article_title.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_vector = np.asarray(bert_model([input_ids_article_title,input_masks_article_title,input_token_id_article_title])[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables_ids = []\n",
    "input_ids_tables_title = []\n",
    "input_masks_tables_title = []\n",
    "input_token_id_tables_title = []\n",
    "\n",
    "for i, row in tqdm(tables.iterrows()):\n",
    "    \n",
    "    table_title = row['table_page_title']\n",
    "    table_page_description = str(row['table_page_summary'])\n",
    "    \n",
    "    #fast text embedding\n",
    "    return_tokenizer1 = bert_tokenizer.encode_plus(\n",
    "      table_title,\n",
    "      table_page_description,\n",
    "      max_length=MAX_TOKENS,\n",
    "      add_special_tokens=True,\n",
    "      return_token_type_ids=True,\n",
    "      pad_to_max_length=True,\n",
    "      return_attention_mask=True,\n",
    "    )\n",
    "    \n",
    "    tables_ids.append(row['table_id'])\n",
    "    \n",
    "    input_ids_tables_title.append(return_tokenizer1['input_ids'])\n",
    "    input_masks_tables_title.append(return_tokenizer1['attention_mask'])\n",
    "    input_token_id_tables_title.append(return_tokenizer1['token_type_ids']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_tables_title = np.array(input_ids_tables_title)\n",
    "input_masks_tables_title = np.array(input_masks_tables_title)\n",
    "input_token_id_tables_title = np.array(input_token_id_tables_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_tables_title.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables_vector = []\n",
    "\n",
    "first = 0\n",
    "\n",
    "for i in tqdm(range(100,(len(input_ids_tables_title)+100),100)):\n",
    "    \n",
    "    tables_vector.append(np.asarray(bert_model([input_ids_tables_title[first:i],input_masks_tables_title[first:i],input_token_id_tables_title[first:i]])[1]))\n",
    "    \n",
    "    first = first + 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables_vector = np.array(tables_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tables_vector_final = tables_vector.reshape(85900,768)\n",
    "tables_vector_final = tables_vector.reshape(-1,768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables_vector_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test the model on final task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAccuracy(idRankedTables, idQueryGoal):\n",
    "\n",
    "    accuracy = 0\n",
    "\n",
    "    for idTable in idRankedTables:\n",
    "        \n",
    "        if idTable[0] == idQueryGoal:\n",
    "    \n",
    "            accuracy = 1\n",
    "            break;\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveAccuracy(k,accuracy):\n",
    "    \n",
    "    if k == 1:\n",
    "            \n",
    "        AverageTop1.append(accuracy)\n",
    "    \n",
    "    if k == 5:\n",
    "            \n",
    "        AverageTop5.append(accuracy)\n",
    "        \n",
    "    if k == 10:\n",
    "            \n",
    "        AverageTop10.append(accuracy)\n",
    "        \n",
    "    if k == 20:\n",
    "            \n",
    "        AverageTop20.append(accuracy)\n",
    "    \n",
    "    if k == 50:\n",
    "            \n",
    "        AverageTop50.append(accuracy)\n",
    "    \n",
    "    if k == 100:\n",
    "            \n",
    "        AverageTop100.append(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AverageTop1 = []\n",
    "AverageTop5 = []\n",
    "AverageTop10 = []\n",
    "AverageTop20 = []\n",
    "AverageTop50 = []\n",
    "AverageTop100 = []\n",
    "\n",
    "topK = [1,5,10,20,50,100]\n",
    "\n",
    "for i in tqdm(range(len(articles_vector))):\n",
    "    \n",
    "    idQueryGoal = articles_ids[i]\n",
    "    \n",
    "    distance_vector = pairwise_distances(articles_vector[i].reshape(1,768), tables_vector_final, metric='cosine')\n",
    "    \n",
    "    #creating the dataframe\n",
    "    all_tables_score = []\n",
    "    \n",
    "    for j in range(len(tables_ids)):\n",
    "        \n",
    "        table_id = tables_ids[j]\n",
    "        table_score = distance_vector[0][j]\n",
    "        \n",
    "        new_row = {\"table_id\": table_id,\"table_score\": table_score}\n",
    "        \n",
    "        all_tables_score.append(new_row)\n",
    "        \n",
    "    df_all_tables_scores = pd.DataFrame(all_tables_score)\n",
    "    df_tables_sorting = df_all_tables_scores.sort_values('table_score')\n",
    "    \n",
    "    #compute the accuracy\n",
    "    for accuracyK in topK:\n",
    "        \n",
    "        selected_top = df_tables_sorting.head(accuracyK)\n",
    "        \n",
    "        min_score = selected_top['table_score'].max()\n",
    "        draw_tables_socres = df_tables_sorting[df_tables_sorting['table_score'] <= min_score]\n",
    "        final_ranked_tables = draw_tables_socres.iloc[:,0:1].values\n",
    "        \n",
    "        accuracy_value = getAccuracy(final_ranked_tables,idQueryGoal)\n",
    "        \n",
    "        #save the accuracy on the list\n",
    "        saveAccuracy(accuracyK,accuracy_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TOP@1 = \"+ str(round(np.mean(AverageTop1),4)))\n",
    "print(\"TOP@5 = \"+ str(round(np.mean(AverageTop5),4)))\n",
    "print(\"TOP@10 = \"+ str(round(np.mean(AverageTop10),4)))\n",
    "print(\"TOP@20 = \"+ str(round(np.mean(AverageTop20),4)))\n",
    "print(\"TOP@50 = \"+ str(round(np.mean(AverageTop50),4)))\n",
    "print(\"TOP@100 = \"+ str(round(np.mean(AverageTop100),4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ancor_title = str(triplet_data.loc[10]['article_page_title'])\n",
    "ancor_description = str(triplet_data.loc[10]['article_page_meta_description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_ancor = []\n",
    "input_masks_ancor = []\n",
    "input_token_id_ancor = []\n",
    "\n",
    "return_tokenizer1 = bert_tokenizer.encode_plus(\n",
    "ancor_title,\n",
    "ancor_description,\n",
    "max_length=MAX_TOKENS,\n",
    "add_special_tokens=True,\n",
    "return_token_type_ids=True,\n",
    "pad_to_max_length=True,\n",
    "return_attention_mask=True)\n",
    "\n",
    "input_ids_ancor.append(return_tokenizer1['input_ids'])\n",
    "input_masks_ancor.append(return_tokenizer1['attention_mask'])\n",
    "input_token_id_ancor.append(return_tokenizer1['token_type_ids']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_ancor = np.array(input_ids_ancor)\n",
    "input_masks_ancor = np.array(input_masks_ancor)\n",
    "input_token_id_ancor = np.array(input_token_id_ancor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ancor_vector = np.asarray(bert_model([input_ids_ancor,input_masks_ancor,input_token_id_ancor])[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ancor_true_title = str(triplet_data.loc[10]['true_table_page_title'])\n",
    "ancor_true_description = str(triplet_data.loc[10]['true_table_page_summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_ancor = []\n",
    "input_masks_ancor = []\n",
    "input_token_id_ancor = []\n",
    "\n",
    "return_tokenizer1 = bert_tokenizer.encode_plus(\n",
    "ancor_true_title,\n",
    "ancor_true_description,\n",
    "max_length=MAX_TOKENS,\n",
    "add_special_tokens=True,\n",
    "return_token_type_ids=True,\n",
    "pad_to_max_length=True,\n",
    "return_attention_mask=True)\n",
    "\n",
    "input_ids_ancor.append(return_tokenizer1['input_ids'])\n",
    "input_masks_ancor.append(return_tokenizer1['attention_mask'])\n",
    "input_token_id_ancor.append(return_tokenizer1['token_type_ids']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_ancor = np.array(input_ids_ancor)\n",
    "input_masks_ancor = np.array(input_masks_ancor)\n",
    "input_token_id_ancor = np.array(input_token_id_ancor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ancor_true_vector = np.asarray(bert_model([input_ids_ancor,input_masks_ancor,input_token_id_ancor])[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.metrics.pairwise import pairwise_kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_ancor_true = pairwise_distances(ancor_vector, ancor_true_vector, metric='cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_ancor_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ancor_false_title = str(triplet_data.loc[10]['false_table_page_title'])\n",
    "ancor_false_description = str(triplet_data.loc[10]['false_table_page_summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_ancor = []\n",
    "input_masks_ancor = []\n",
    "input_token_id_ancor = []\n",
    "\n",
    "return_tokenizer1 = bert_tokenizer.encode_plus(\n",
    "ancor_false_title,\n",
    "ancor_false_description,\n",
    "max_length=MAX_TOKENS,\n",
    "add_special_tokens=True,\n",
    "return_token_type_ids=True,\n",
    "pad_to_max_length=True,\n",
    "return_attention_mask=True)\n",
    "\n",
    "input_ids_ancor.append(return_tokenizer1['input_ids'])\n",
    "input_masks_ancor.append(return_tokenizer1['attention_mask'])\n",
    "input_token_id_ancor.append(return_tokenizer1['token_type_ids']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_ancor = np.array(input_ids_ancor)\n",
    "input_masks_ancor = np.array(input_masks_ancor)\n",
    "input_token_id_ancor = np.array(input_token_id_ancor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ancor_false_vector = np.asarray(bert_model([input_ids_ancor,input_masks_ancor,input_token_id_ancor])[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_ancor_false = pairwise_distances(ancor_vector, ancor_false_vector, metric='cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"distance to true \"+str(distance_ancor_true))\n",
    "print(\"distance to false \"+str(distance_ancor_false))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text1 = \"i love you\"\n",
    "sample_text2 = \"glory finally nabs dodd league the world game sbs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_tokenizer1 = bert_tokenizer.encode_plus(\n",
    "sample_text1,\n",
    "sample_text2,\n",
    "max_length=30,\n",
    "add_special_tokens=True,\n",
    "return_token_type_ids=True,\n",
    "pad_to_max_length=True,\n",
    "return_attention_mask=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_tokenizer1['input_ids'])\n",
    "return_tokenizer1['attention_mask']\n",
    "return_tokenizer1['token_type_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_tokenizer1['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_tokenizer1['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_tokenizer1['token_type_ids']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
