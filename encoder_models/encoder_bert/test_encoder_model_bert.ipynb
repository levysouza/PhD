{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import random\n",
    "import gensim as gs\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer()\n",
    "import tensorflow as tf\n",
    "from transformers import TFBertModel, BertTokenizer, TFBertMainLayer, BertConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "    raise SystemError('GPU not found')\n",
    "print('found GPU at {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = TFBertModel.from_pretrained(\"bert-base-cased\")\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TOKENS = 125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = pd.read_csv('../../dataset/data_articles_test.csv', delimiter=',', keep_default_na=False)\n",
    "\n",
    "tables = pd.read_csv('../../dataset/distinct_tables_allsignals.csv', delimiter=',', keep_default_na=False)\n",
    "tables = tables.drop(tables[tables.table_page_title == \"\"].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplet_loss(y_true, y_pred, alpha = 0.5):\n",
    "     \n",
    "    anchor = y_pred[:,0:768]\n",
    "    positive = y_pred[:,768:1536]\n",
    "    negative = y_pred[:,1536:2304]\n",
    "        \n",
    "    # distance between the anchor and the positive\n",
    "    pos_dist = tf.keras.layers.Dot(axes=1,normalize=True)([anchor, positive])\n",
    "    \n",
    "    # distance between the anchor and the negative\n",
    "    neg_dist = tf.keras.layers.Dot(axes=1,normalize=True)([anchor, negative])\n",
    "    \n",
    "    # compute loss\n",
    "    basic_loss = (1 - pos_dist) - (1 - neg_dist) + alpha\n",
    "    loss = tf.keras.backend.maximum(basic_loss,0.0)\n",
    " \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = tf.keras.models.load_model('bert_encoder_model2', custom_objects={'triplet_loss': triplet_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_name = 'bert'\n",
    "intermediate_layer_model = tf.keras.Model(inputs=[loaded_model.input[0],loaded_model.input[1],loaded_model.input[2]],\n",
    "                                 outputs=loaded_model.get_layer(layer_name).output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## encode articles using encoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_ids = []\n",
    "input_ids_article_title = []\n",
    "input_masks_article_title = []\n",
    "input_token_id_article_title = []\n",
    "\n",
    "for i, row in tqdm(articles.iterrows()):\n",
    "    \n",
    "#     article_catch_all = str(row['page_title'])+\" \"+ str(row['meta_description'])\n",
    "    article_catch_all = str(row['page_title']) +\" \"+ str(row['meta_description'])+\" \"+str(row['keywords'])\n",
    "      \n",
    "    #fast text embedding\n",
    "    return_tokenizer1 = bert_tokenizer.encode_plus(\n",
    "      article_catch_all,\n",
    "      max_length=MAX_TOKENS,\n",
    "      add_special_tokens=True,\n",
    "      return_token_type_ids=True,\n",
    "      pad_to_max_length=True,\n",
    "      return_attention_mask=True,\n",
    "    )\n",
    "    \n",
    "    #save list\n",
    "    articles_ids.append(row['page_id'])\n",
    "    \n",
    "    input_ids_article_title.append(return_tokenizer1['input_ids'])\n",
    "    input_masks_article_title.append(return_tokenizer1['attention_mask'])\n",
    "    input_token_id_article_title.append(return_tokenizer1['token_type_ids']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_article_title = np.array(input_ids_article_title)\n",
    "input_masks_article_title = np.array(input_masks_article_title)\n",
    "input_token_id_article_title = np.array(input_token_id_article_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_token_id_article_title.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_vector = np.asarray(intermediate_layer_model.predict([input_ids_article_title,input_masks_article_title,input_token_id_article_title])[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables_ids = []\n",
    "input_ids_tables_title = []\n",
    "input_masks_tables_title = []\n",
    "input_token_id_tables_title = []\n",
    "\n",
    "for i, row in tqdm(tables.iterrows()):\n",
    "    \n",
    "#     table_catch_all = str(row['table_page_title'])+\" \"+str(row['table_page_summary'])\n",
    "    table_catch_all = str(row['table_page_title'])+\" \"+str(row['table_page_summary'])+\" \"+str(row['table_page_keywords'])\n",
    "    \n",
    "    #fast text embedding\n",
    "    return_tokenizer1 = bert_tokenizer.encode_plus(\n",
    "      table_catch_all,\n",
    "      max_length=MAX_TOKENS,\n",
    "      add_special_tokens=True,\n",
    "      return_token_type_ids=True,\n",
    "      pad_to_max_length=True,\n",
    "      return_attention_mask=True,\n",
    "    )\n",
    "    \n",
    "    tables_ids.append(row['table_id'])\n",
    "    \n",
    "    input_ids_tables_title.append(return_tokenizer1['input_ids'])\n",
    "    input_masks_tables_title.append(return_tokenizer1['attention_mask'])\n",
    "    input_token_id_tables_title.append(return_tokenizer1['token_type_ids']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_tables_title = np.array(input_ids_tables_title)\n",
    "input_masks_tables_title = np.array(input_masks_tables_title)\n",
    "input_token_id_tables_title = np.array(input_token_id_tables_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_tables_title.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables_vector = []\n",
    "\n",
    "first = 0\n",
    "\n",
    "for i in tqdm(range(100,(len(input_ids_tables_title)+100),100)):\n",
    "    \n",
    "    tables_vector.append(np.asarray(intermediate_layer_model.predict([input_ids_tables_title[first:i],input_masks_tables_title[first:i],input_token_id_tables_title[first:i]])[1]))\n",
    "    \n",
    "    first = first + 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables_vector = np.array(tables_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tables_vector_final = tables_vector.reshape(85900,768)\n",
    "tables_vector_final = tables_vector.reshape(-1,768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables_vector_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test the model on final task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAccuracy(idRankedTables, idQueryGoal):\n",
    "\n",
    "    accuracy = 0\n",
    "\n",
    "    for idTable in idRankedTables:\n",
    "        \n",
    "        if idTable[0] == idQueryGoal:\n",
    "    \n",
    "            accuracy = 1\n",
    "            break;\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveAccuracy(k,accuracy):\n",
    "    \n",
    "    if k == 1:\n",
    "            \n",
    "        AverageTop1.append(accuracy)\n",
    "    \n",
    "    if k == 5:\n",
    "            \n",
    "        AverageTop5.append(accuracy)\n",
    "        \n",
    "    if k == 10:\n",
    "            \n",
    "        AverageTop10.append(accuracy)\n",
    "        \n",
    "    if k == 20:\n",
    "            \n",
    "        AverageTop20.append(accuracy)\n",
    "    \n",
    "    if k == 50:\n",
    "            \n",
    "        AverageTop50.append(accuracy)\n",
    "    \n",
    "    if k == 100:\n",
    "            \n",
    "        AverageTop100.append(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AverageTop1 = []\n",
    "AverageTop5 = []\n",
    "AverageTop10 = []\n",
    "AverageTop20 = []\n",
    "AverageTop50 = []\n",
    "AverageTop100 = []\n",
    "\n",
    "topK = [1,5,10,20,50,100]\n",
    "\n",
    "for i in tqdm(range(len(articles_vector))):\n",
    "    \n",
    "    idQueryGoal = articles_ids[i]\n",
    "    \n",
    "    distance_vector = pairwise_distances(articles_vector[i].reshape(1,768), tables_vector_final, metric='cosine')\n",
    "    \n",
    "    #creating the dataframe\n",
    "    all_tables_score = []\n",
    "    \n",
    "    for j in range(len(tables_ids)):\n",
    "        \n",
    "        table_id = tables_ids[j]\n",
    "        table_score = distance_vector[0][j]\n",
    "        \n",
    "        new_row = {\"table_id\": table_id,\"table_score\": table_score}\n",
    "        \n",
    "        all_tables_score.append(new_row)\n",
    "        \n",
    "    df_all_tables_scores = pd.DataFrame(all_tables_score)\n",
    "    df_tables_sorting = df_all_tables_scores.sort_values('table_score')\n",
    "    \n",
    "    #compute the accuracy\n",
    "    for accuracyK in topK:\n",
    "        \n",
    "        selected_top = df_tables_sorting.head(accuracyK)\n",
    "        \n",
    "        min_score = selected_top['table_score'].max()\n",
    "        draw_tables_socres = df_tables_sorting[df_tables_sorting['table_score'] <= min_score]\n",
    "        final_ranked_tables = draw_tables_socres.iloc[:,0:1].values\n",
    "        \n",
    "        accuracy_value = getAccuracy(final_ranked_tables,idQueryGoal)\n",
    "        \n",
    "        #save the accuracy on the list\n",
    "        saveAccuracy(accuracyK,accuracy_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TOP@1 = \"+ str(round(np.mean(AverageTop1),4)))\n",
    "print(\"TOP@5 = \"+ str(round(np.mean(AverageTop5),4)))\n",
    "print(\"TOP@10 = \"+ str(round(np.mean(AverageTop10),4)))\n",
    "print(\"TOP@20 = \"+ str(round(np.mean(AverageTop20),4)))\n",
    "print(\"TOP@50 = \"+ str(round(np.mean(AverageTop50),4)))\n",
    "print(\"TOP@100 = \"+ str(round(np.mean(AverageTop100),4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idQueryGoal"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
